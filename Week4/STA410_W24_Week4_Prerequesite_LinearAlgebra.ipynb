{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YymWs_lQA0_x"
   },
   "source": [
    "<a name=\"cell-solving\"></a>\n",
    "\n",
    "\n",
    "0. [Linearly Transforming](#cell-transforming-Axb) [$x$](#cell-transforming-Axb) [with](#cell-transforming-Axb) [$Ax=b$](#cell-transforming-Axb)\n",
    "  1. [[PREREQUESITE] Linear Independence, Orthogonality, Rank, and Bases](#cell-Axb-etc): Week 2 Programming Assignment Problem 1\n",
    "  2. [[PREREQUESITE] Eigenvalues and Eigenvectors](#cell-sovling-Axb-math-eig2)\n",
    "  3. [[PREREQUESITE] Eigendecomposition](#cell-sovling-Axb-math-eig1)\n",
    "  4. [Understanding](#cell-sovling-Axb-Eigenanalysis) [$Ax$](#cell-sovling-Axb-Eigenanalysis) [by its Eigenvalues](#cell-sovling-Axb-Eigenanalysis)\n",
    " \n",
    "3. [Solving for](#cell-sovling-Axb) [$x$](#cell-sovling-Axb) [in](#cell-sovling-Axb) [$Ax=b$](#cell-sovling-Axb)\n",
    "  1. [$A^{-1}$](#cell-sovling-AxbwAinv)\n",
    "    1. [[POSTPONED?] Sherman-Morrison-Woodbury Formula](#cell-sovling-SMWoodbury)\n",
    "    2. [[OMITTED] Generalized Inverses](#cell-sovling-inverses)\n",
    "  2. [Not](#cell-sovling-notAxbwAinv) [$A^{-1}$](#cell-sovling-notAxbwAinv)\n",
    "    1. [[PREREQUISITE] Backward Substitution](#cell-sovling-backsub)\n",
    "    2. [[PREREQUISITE] Gaussian Elimination](#cell-sovling-elimination)\n",
    "    3. [[PREREQUISITE] Elementary Operations](#cell-sovling-elementary)\n",
    "    4. [The LU Decomposition](#cell-sovling-lu)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "LbvCD6r5qkec"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "# https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.multivariate_normal.html\n",
    "import statsmodels.api as sm\n",
    "# https://www.statsmodels.org/dev/datasets/index.html\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor as VIF\n",
    "# https://en.wikipedia.org/wiki/Variance_inflation_factor#Calculation_and_analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bbj3fBvpAnQG"
   },
   "source": [
    "<a name=\"first_cell\"></a>\n",
    "\n",
    "# 3. $Ax=b$ and Singular Values ([Return to TOC](#cell-solving))\n",
    "\n",
    "---\n",
    "\n",
    "Linear algebra is concerned with [linear transformations](https://www.mathsisfun.com/algebra/matrix-transform.html) of the form $Ax = b$, and can be viewed as a set of terminologies and notational rules capturing useful concepts and computational operations related to this transformation. One of the centrally useful concepts in linear algebra analysis is ***singular values***. \n",
    "\n",
    "Many of the results in statistics are based on specifications representable within the linear algebra framework, so to build a mature level of understanding of statistical methodology it is imperitive to develop comfort and understanding of the underlying linear algebra concepts and notations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uk-g6ERTCFD3"
   },
   "source": [
    "<a name=\"cell-transforming-Axb\"></a>\n",
    "\n",
    "# 3.0 Linearly Transforming $x$ with $Ax=b$ ([Return to TOC](#cell-solving))\n",
    "\n",
    "---\n",
    "\n",
    "The first interpretation of $A_{n\\times m}x_{m\\times 1} = b_{n\\times 1}$ is that $b$ is a point in $n$-dimensional space which can be expressed in terms of the coordinates $x_j$ with respect to the axes of $m$-dimensional space defined by the column vectors $A_{\\cdot j}$ of $A$. That is \n",
    "\n",
    "$$b = \\underset{\\text{$e_i$ are the so-called standard basis vectors}}{b_1 \\left[ \\begin{array}{c}1\\\\0\\\\\\vdots\\\\0 \\end{array}\\right] + b_2 \\left[ \\begin{array}{c}0\\\\1\\\\\\vdots\\\\0 \\end{array}\\right] + \\cdots + b_n \\left[ \\begin{array}{c}0\\\\0\\\\\\vdots\\\\1 \\end{array}\\right]} = \\sum_{i=1}^m b_i e_{i} = \\sum_{j=1}^m x_j A_{\\cdot j} = Ax$$\n",
    "\n",
    "The $A$ matrix specifies the way $x$ is transformed into $b$; or, given $b$, the task may be to find $x$ the alternative coordinate representation of $b$. The inverse problem of solving for $x$ turns out to be very useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "XnaRGmWlElIT"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/sd/hnfh4zsn34d7xpbz9226pz200000gn/T/ipykernel_48959/3212854165.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# so we can do a DFT of a vector x\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"x =\\n\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\n\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mDFT_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m@\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"DFT(x) =\\n\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDFT_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\n\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "# the following code gives examples of matrix operations in python\n",
    "A = np.ones((3,3)); x = np.ones((3,1))\n",
    "A@x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1rM_biG_ElO7"
   },
   "outputs": [],
   "source": [
    "A.dot(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I7m25Ia3ElUs"
   },
   "outputs": [],
   "source": [
    "x.dot(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GGYqjI8jEsUP"
   },
   "outputs": [],
   "source": [
    "y = np.array([[2],[2],[2]])\n",
    "# x and y are not linearly independent, since the following is zero.\n",
    "y - 2*x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VcZeShG8FsgR"
   },
   "outputs": [],
   "source": [
    "# x and y are not orthogonal since their dot product isn't zero\n",
    "x.T.dot(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XTwNM7XZF1UU"
   },
   "outputs": [],
   "source": [
    "xy = np.concatenate([x,y], axis=1)\n",
    "xy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bOcQRlnfFqUI"
   },
   "outputs": [],
   "source": [
    "# the rank of xy is 1 since x and y are linearly dependent\n",
    "np.linalg.matrix_rank(xy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lXbhi_ptEsWp"
   },
   "outputs": [],
   "source": [
    "e1,e2 = np.array([[1],[0],[0]]), np.array([[0],[1],[0]])\n",
    "# e1 is a normal vector and is orthogonal to e2 since the following are 0 and 1\n",
    "e1.T.dot(e1), e1.T.dot(e2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ukVPhknIFZ5E"
   },
   "outputs": [],
   "source": [
    "# the rank of [e1,e2] is 2 since are orthogonal, and hence  linearly independent\n",
    "np.linalg.matrix_rank(np.c_[e1,e2]) # different way to concatenate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K6FlESBmDqW5"
   },
   "source": [
    "<a name=\"cell-Axb-etc\"></a>\n",
    "\n",
    "## [PREREQUESITE] Linear Independence, Orthogonality, Rank, and Bases ([Return to TOC](#cell-solving))\n",
    "\n",
    "---\n",
    "\n",
    "To uniquely define $b$ above in terms of axes defined by the columns of $A$, it must be the case that that $A_{\\cdot j}\\not =c A_{\\cdot k}$ for all $j \\neq k$ so that no two axes redunantly point in the same direction.  This will be the case if the columns $A_{\\cdot j}$ are ***linearly indepdendent*** so that\n",
    "\n",
    "  $$ \\underbrace{\\sum_{j = 1}^n c_j A_{\\cdot j} = 0  \\;\\; \\Longrightarrow  \\;\\; c_j = 0 \\text{ for all } j}_{Ac \\;=\\; 0 \\;\\;\\Longrightarrow \\;\\;c \\;=\\; 0}$$\n",
    "\n",
    "A stronger condition than ***linear independence*** is ***orthogonality*** where \n",
    "\n",
    "$$ (A_{\\cdot j})^T A_{\\cdot k} =  \\sum_i A_{ij} A_{ik} = 0 \\text{ for all } j \\neq k \\quad \\text{ and } \\quad (A_{\\cdot j\n",
    "})^T A_{\\cdot k} \\neq 0 \\text{ for all } j$$\n",
    "\n",
    "since for nonzero columns $A_{\\cdot j}$ \n",
    "\n",
    "$$  \\underbrace{(A_{\\cdot j})^T A_{\\cdot k} = 0}_{\\text{Orthogonality}} \\quad \\Longrightarrow \\quad \\underbrace{c_jA_{\\cdot j} + c_kA_{\\cdot k} = 0 \\Longrightarrow c_j=c_k=0}_{\\text{Linear Independence}}$$\n",
    "\n",
    "but \n",
    "\n",
    "$$\\require{\\cancel} \\underbrace{c_jA_{\\cdot j} + c_kA_{\\cdot k} = 0 \\Longrightarrow c_j=c_k=0}_{\\text{Linear Independence}} \\quad \\cancel{\\Longrightarrow} \\quad  \\underbrace{(A_{\\cdot j})^T A_{\\cdot k} = 0}_{\\text{Orthogonality}}$$\n",
    "\n",
    "- It is possible to transform two ***linearly independent*** columns $A_{\\cdot j}$ and $A_{\\cdot k}$ so that they are ***orthogonal***, and the most common way to do this is known as the ***Gram-Schmidt procedure***. Implementation of the ***Gram-Schmidt procedure*** is addressed in the [Week 3 Programming Assignment Problem 1]().\n",
    "\n",
    "The ***rank*** of the matrix $A_{n\\times m}$ is the number of ***linearly independent*** columns (and equivalently, rows) of $A$. The matrix $A$ is said to be ***full rank*** if $\\text{rank}(A_{n \\times m}) = \\min(n,m)$.  When $A$ is ***square*** so $A_{n\\times m} = A_{n\\times n}$, if $A$ is ***full rank*** then $\\text{rank}(A_{n \\times n}) = n$ and the $n$ columns of a $A_{n \\times n}$ are ***linearly independent*** and form a ***basis*** in $n$-dimensional space. \n",
    "\n",
    "- A ***basis*** formed by the $n$ ***linearly independent*** columns of a ***square*** matrix $A$ is a set of axes defining a coordinate system from which to index the $n$-dimensional space.  \n",
    "\n",
    "  >Any vector of an $n$-dimensional space $x$ may be given in terms of the coordinates of any ***basis*** formed by a ***full rank square matrix*** as \n",
    ">\n",
    ">$$b = \\sum_j c_j A_{\\cdot j}$$\n",
    ">\n",
    ">which illustrates that a ***basis*** does not define the space; rather, the ***basis*** just defines the way points $x$ in the space are referenced.\n",
    "Changing the ***basis*** does not change the space itself. \n",
    "\n",
    "The ***standard basis*** is $A_{n \\times n}=I$. The columns of $I$ are the ***standard basis vectors*** $e_j$.  The $e_j$ are ***linearly independent*** and ***orthogonal***; and, because the length of these vectors in the n-dimensional space is 1, they are called ***normal vectors***. \n",
    "\n",
    ">The (***Euclidean distance***) length of a vector is given by its the square root of its ***inner (dot) product*** with itself, so a column vector $A_{\\cdot j}$ is a ***normal vector*** if \n",
    ">\n",
    "> $$\\sqrt{A_{\\cdot j} \\cdot A_{\\cdot j}} = \\sqrt{(A_{\\cdot j})^T A_{\\cdot j}} = \\sqrt{\\sum_{j = 1}^n A_{i j}^2} = 1 \\quad \\text{ e.g., } \\quad e_j \\cdot e_j = e_j^T e_j =1$$ \n",
    "\n",
    "Vectors which are both ***normal*** and ***orthogonal*** are called ***orthonormal***. The ***standard basis*** is thus an ***orthonormal basis***. Two standard convensions that are common in this context are\n",
    "\n",
    "1. since two vectors are ***linearly independent*** regardless of their length, it is usual to specify the vectors of a ***basis*** in their ***normal form***, and\n",
    "2. an ***orthonormal basis*** is often just called an ***orthogonal basis*** as the ***orthogonality*** is a much more crucial property of such a basis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kwyt2898P5UC"
   },
   "source": [
    "<a name=\"cell-sovling-Axb-math-eig2\"></a>\n",
    "\n",
    "## [PREREQUESITE] Eigenvalues and Eigenvectors ([Return to TOC](#cell-solving))\n",
    "\n",
    "---\n",
    "\n",
    "***Eigenvalue*** and ***eigenvector*** analysis of the linear transformation $A_{n\\times n}$ examines the rate of the expansion (and/or contraction) along the invariant directions of the transformation, respectively, as\n",
    "\n",
    "$$A_{n\\times n} V_{\\cdot j} = \\lambda_j V_{\\cdot j} \\quad \\text{often usefully encountered as} \\quad (A_{n\\times n} - \\lambda_j I) V_{\\cdot j} = 0$$\n",
    "\n",
    "Thus, for $x = \\sum_{j} c_j V_{\\cdot j}$ expressed in an ***eigenvector basis*** (regardless of the ***rank*** of $A_{n\\times n}$) \n",
    "\n",
    "   $$Ax= A\\left(\\sum_{j} c_j V_{\\cdot j}\\right) = \\sum_{j} c_j A  V_{\\cdot j} = \\sum_{j} c_j \\lambda_j V_{\\cdot j}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z2jSVs9tjk0N"
   },
   "source": [
    "<a name=\"cell-sovling-Axb-math-eig1\"></a>\n",
    "\n",
    "## [PREREQUESITE] Eigendecomposition ([Return to TOC](#cell-solving))\n",
    "\n",
    "---\n",
    "\n",
    "Any matrix $\\Sigma$ such that\n",
    "\\begin{align*}\n",
    "\\Sigma =  {}& \\Sigma^T & \\textbf{symmetric}\\\\\n",
    "x^T\\Sigma x {}& > 0 & \\textbf{positive definite}\\\\ \n",
    "\\end{align*}\n",
    "\n",
    "is ***full rank*** (so $\\text{rank}(\\Sigma_{n\\times n}) = n$) and may be a ***covariance matrix***. For such matrices, there exists an ***eigendecomposition*** (or synonymously, ***spectral decomposition*** or ***diagonal factorization***)\n",
    "\n",
    "\\begin{align*}\n",
    "\\Sigma_{n\\times n} = {} & V_{n\\times n} \\Lambda_{n\\times n} (V^T)_{n\\times n}\\\\\n",
    "\\end{align*}\n",
    "\n",
    "such that  \n",
    "- ***orthonormal eigenvectors*** of $\\Sigma$ form the columns of the ***orthonormal matrix*** $V_{n\\times n}$  \n",
    "\n",
    "\n",
    "  $$\\begin{align*}\n",
    "  V_{\\cdot j}^TV_{\\cdot j} & {} = \\,\\;1\\;\\,  = V_{j\\cdot}^TV_{j\\cdot} & {} \\textbf{normal vectors}\\\\\n",
    "  V_{\\cdot j}^TV_{\\cdot k} & {} = \\,\\;0\\;\\,  = V_{j\\cdot}^TV_{k\\cdot}, j\\not=k & {} \\textbf{orthogonality}\\\\\n",
    "  V^TV & {} = I_{n\\times n}  = VV^T & {} \\textbf{orthonormality}\n",
    "  \\end{align*}$$\n",
    "\n",
    "- and corresponding positive ***eigenvalues*** \n",
    "\n",
    "  $$\\Lambda_{11}=\\lambda_1 \\geq \\Lambda_{22}=\\lambda_2 \\geq \\cdots \\geq \\Lambda_{nn}=\\lambda_n > 0$$\n",
    "\n",
    "  comprise the entries of the diagonal matrix $\\Lambda_{n\\times n}$ \n",
    "\n",
    "> The case of ***symmetric positive definite*** is quite distinct compared to more general ***eigendecomposition***. \n",
    ">\n",
    "> [***Eigendecomposition***](https://en.wikipedia.org/wiki/Eigendecomposition_of_a_matrix) exists more generally for (***square***) [***diagnalizable matrices***](https://math.stackexchange.com/questions/1811983/diagonalizable-vs-full-rank-vs-nonsingular-square-matrix) which might be neither ***positive definite*** nor ***symmetric***. In this case, the  ***eigendecomposition*** is $V\\Lambda V^{-1}$ where $V^{-1}\\neq V^T$, and the  ***eigenvalues*** may not all be positive and the ***eigenvectors*** may not all be ***orthogonal***:\n",
    "- the ***eigenvectors*** are ***orthogonal*** when $\\Sigma$ is ***symmetric*** since this means $V^{-1} = V^T$\n",
    "- the ***eigenvalues*** $\\lambda_i > 0$ of $\\Sigma$ are positive when ***symmetric*** $\\Sigma$ is **positive definite**\n",
    ">\n",
    "> ***Eigendecomposition*** also exists for ***diagonalizable matrices*** which are not ***full rank***. In this case, $r>0$ ***eigenvalues*** will be nonzero $\\Lambda_{ii} = \\lambda_{i}$ for $i\\leq n-r$ and $\\Lambda_{ii} = 0$ for $r < i \\leq n$ in the diagonal matrix $\\Lambda$. The ***eigendecomposition*** then has the ***compact*** form\n",
    ">\n",
    "> $$A_{n\\times n} = V_{n \\times n} \\Lambda_{n \\times n} V^{-1}_{n \\times n} = V_{n \\times r} \\Lambda_{r \\times r} V^{-1}_{r \\times n}$$\n",
    "> \n",
    "> and the columns of $V_{n \\times r}$ will be ***linearly independent*** [so long as](https://math.stackexchange.com/questions/157382/are-the-eigenvectors-of-a-real-symmetric-matrix-always-an-orthonormal-basis-with) all ***non-zero eigenavalues are unique*** (i.e., have multiplicity $1$). The remaining columns in $V^T_{n \\times n}$ may be chosen arbitrarily, e.g., to also be ***linearly independent*** since they will not contribute to $V \\Lambda V^T$ for any diagonal element $\\Lambda_{ii} = 0$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cj7gjSNSaFz-"
   },
   "source": [
    "<a name=\"cell-sovling-Axb-Eigenanalysis\"></a>\n",
    "\n",
    "## Understanding $Ax$ by its Eigenvalues ([Return to TOC](#cell-solving))\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "***Eigenvalues*** determine many properties of an $A_{n \\times n}$ matrix.\n",
    "\n",
    "1. The ***determinant*** of the matrix $A_{n \\times n}$ is the product of the ***eigenvalues*** \n",
    "\n",
    "   $$\\det(A_{n\\times n}) = \\prod_{i=1}^n \\lambda_i$$ \n",
    "\n",
    "   and so characterizes the multiplicative change in the \"geometric volume\" of the space under the linear transformation $Ax$.\n",
    "\n",
    "2. The ***spectral radius*** of $A_{n\\times n}$ is the largest absolute ***eigenvalue***\n",
    "\n",
    "   $$\\rho(A_{n\\times n}) = \\underset{i=1,...,n}{\\max} |\\lambda_i| \\leq \\begin{array}{c}\\underset{i=1,...,n}{\\max} \\sum_{j=1}^n |A_{ij}| \\\\ \\underset{j=1,...,n}{\\max} \\sum_{i=1}^n |A_{ij}|\\end{array}$$\n",
    "   \n",
    "   which represents the maximul \"radius\" of the transformation of the space under $A_{n\\times n}$ and influences many statistical and computational characteristics of $A_{n\\times n}$.\n",
    "\n",
    "3. The ***trace*** (sum of diagonal elements) of $A_{n\\times n}$ is the sum of the ***eigenvalues*** \n",
    "  \n",
    "   $$\\text{tr}(A_{n\\times n}) = \\sum_{i=1}^n A_{ii} = \\sum_{i=1}^n \\lambda_i$$\n",
    "\n",
    "   so, e.g., the \"total variance\" (sum of the diagonal elements) of a ***covariance matrix*** is the sum of the ***eigenvalues*** of the covariance matrix.\n",
    "\n",
    "<!--\n",
    "   > which can be shown using the [Jordan canonical form](https://math.stackexchange.com/questions/546155/proof-that-the-trace-of-a-matrix-is-the-sum-of-its-eigenvalues) $A=P J P^{-1}$ (whose diagonal elements $J_{ii}$ are the ***eigenvalues*** of $A$) and the cyclical $\\text{trace}(AB)=\\text{trace}(BA)$ property of the ***trace*** operator \n",
    "   >\n",
    "   > $$\\begin{align*}\\text{tr}(A) = {} & \\text{tr}(PJP^{-1}) = \\text{tr}(JP^{-1}P) = \\text{tr}(J) = \\sum_{i=1}^n J_{ii} = \\sum_{i=1}^n \\lambda_i \\end{align*}$$\n",
    "   -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I5dRo09SOYno"
   },
   "source": [
    "<a name=\"cell-sovling-Axb\"></a>\n",
    "\n",
    "# 3.3 Solving for $x$ in $Ax=b$ ([Return to TOC](#cell-solving))\n",
    "\n",
    "---\n",
    "\n",
    "When $A^{-1}$ exists so that there is a solution $x$ for $Ax=b$, rather than computing \n",
    "\n",
    "- $x=A^{-1}b$ with `np.linalg.inv(A) @ b` \n",
    "\n",
    "a better solution  \n",
    "\n",
    "- solving for $x$ in $Ax=b$ with `np.linalg.solve(A, b)`\n",
    "\n",
    "does not require explicitly computing $A^{-1}$, and it is faster to not do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YW2TWRRZOjX_"
   },
   "outputs": [],
   "source": [
    "n = 1000\n",
    "A, b = stats.norm.rvs(size=(n,n)), stats.norm.rvs(size=(n,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4ybmFLjLOjdd"
   },
   "outputs": [],
   "source": [
    "%timeit np.linalg.inv(A) @ b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kgcaIqiPOjjk"
   },
   "outputs": [],
   "source": [
    "# 3 times faster (not n times faster)... we'll return to this later\n",
    "%timeit np.linalg.solve(A, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nsu1XMIfOvEl"
   },
   "outputs": [],
   "source": [
    "# and of course if A isn't invertible, then...\n",
    "np.linalg.inv(np.ones((n,n)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K9S2NvdSBajX"
   },
   "source": [
    "<a name=\"cell-sovling-AxbwAinv\"></a>\n",
    "\n",
    "## 3.3.0 $A^{-1}$ ([Return to TOC](#cell-solving))\n",
    "\n",
    "---\n",
    "\n",
    "If we write $x=A^{-1}b$ as the solution to the system of linear equations $Ax = b$ we are implying that $A$ is an $n\\times n$ square matrix which is ***full rank*** or (synonymously) ***invertible or nonsingular***. $A^{-1}$ doesn't exist for ***non full rank*** or (synonymously) ***non-invertible or singular*** matrices.\n",
    "\n",
    "> The use of the synonym ***nonsingular*** in place of the more straightforward term ***invertible*** is because if $A^{-1}$ does not exist, then the ***inverse function***\n",
    ">\n",
    "> $$f(A) = A^{-1}$$\n",
    ">\n",
    "> is not defined at $A$ and so then $A$ is a point of [mathematical singularity](https://en.wikipedia.org/wiki/Singularity_(mathematics)) in the ***domain*** of $f$. \n",
    "\n",
    "$A_{n\\times n}^{-1} = $ [$\\det(A_{n\\times n})^{-1}\\operatorname {adj}(A_{n\\times n})$](https://en.wikipedia.org/wiki/Adjugate_matrix#Definition) and the ***determinant*** is the product of the ***eigenvalues*** of $A$ a well as the product of the (absolute) ***singular values*** of $A$. Thus, for ***singular values*** (and ***eigenvalues***) of $A_{n\\times n}, \\lambda_j, 1 \\leq j \\leq n$, if\n",
    "\n",
    "| $\\lambda_j = 0$ for some $j$| $\\lambda_j \\not = 0$ for all $j$ |\n",
    "|-|-|\n",
    "| $\\det A = 0$ | $\\det A \\neq 0$ |\n",
    "| division by $0$ | no division by $0$ | \n",
    "|$A$ is ***singular*** | $A$ is ***nonsingular*** |\n",
    "| $A$ is ***not invertible*** | $A$ is ***invertible*** |\n",
    "| $A$ is ***not full rank*** | $A$ is ***full rank*** |\n",
    "| Some columns (rows) are | All columns (rows) are\n",
    "| ***linearly dependent*** | ***linearly independent*** |\n",
    "| $A^{-1}$ does not exist | $A^{-1}$ exists |\n",
    "\n",
    "Even if $A^{-1}$ exists, there are three problems:\n",
    "\n",
    "0. Inverse computation is **(usually)** not a simple algorithm, like ***transpose*** $A^T$ \n",
    "  - which just reverse the indexing scheme $\\quad[A^T]_{ij} = A_{ji}$\n",
    "  - and have simple higher order properties $\\quad (AB)^T = B^TA^T$\n",
    "\n",
    "    > However, notice that for ***orthonormal*** matrices (which are often simply just referred to as ***orthogonal*** matrices since the columns can be easily ***standardized*** into ***normal vectors***)\n",
    "    > $$W_{n \\times n}^TW_{n \\times n}=W_{n \\times n}W_{n \\times n}^T = I_{n \\times n}.$$    \n",
    "    >\n",
    "    > and **inversion*** is ***transposition**; and, this is partially true for \n",
    "    > ***semi-orthogonal*** (or ***semi-orthonormal***) matrices where \n",
    "    >\n",
    "    >   $$S_{n \\times m}^TS_{n \\times m}=I_{m \\times m}$$\n",
    "\n",
    "1. Inverse computation is unnecessarily wasteful\n",
    "  - since solving for $x$ in $Ax = b$ means solving $$A \\left[\\begin{array}{c}x_1\\\\\\vdots\\\\x_n\\end{array}\\right] = \\left[\\begin{array}{c}b_1\\\\\\vdots\\\\b_n\\end{array}\\right]$$\n",
    "  - but computing $x=A^{-1}b$ means either knowing or solving for $A^{-1}$\n",
    "  $$A \\left[\\!\\!\\!\\!\\!\\!\\begin{array}{c:c:c:c} & \n",
    "  \\begin{array}{c}A_{11}^{-1}\\\\\\vdots\\\\A_{n1}^{-1}\\end{array}& \n",
    "  \\begin{array}{c}A_{12}^{-1}\\\\\\vdots\\\\A_{n2}^{-1}\\end{array}&\\cdots&\n",
    "  \\begin{array}{c}A_{1n}^{-1}\\\\\\vdots\\\\A_{nn}^{-1}\\end{array}& \n",
    "  \\end{array}\\!\\!\\!\\!\\!\\!\\right] = \n",
    "  \\left[\\!\\!\\!\\!\\!\\!\\begin{array}{c:c:c:c} & \n",
    "  \\begin{array}{c}1\\\\0\\\\\\vdots\\\\\\vdots\\\\0\\end{array}& \n",
    "  \\begin{array}{c}0\\\\1\\\\0\\\\\\vdots\\\\0\\end{array}&\\cdots&\n",
    "  \\begin{array}{c}0\\\\\\vdots\\\\\\vdots\\\\0\\\\1\\end{array}& \n",
    "  \\end{array}\\!\\!\\!\\!\\!\\!\\right]$$\n",
    "\n",
    "     which requires solving the $n$ equations $AA_{\\cdot j}^{-1} = e_{j}$ for $j = 1, \\cdots, n$ for $A_{\\cdot j}^{-1}$ where $A_{\\cdot j}^{-1}$ is the $j^{th}$ column of $A^{-1}$ and $e_{j}$ is the ***standard basis vector*** with all elements equal to $0$ except the $j^{th}$ element which is equal to $1$.\n",
    "\n",
    "2. Inversion computation is actually very often prone to numerical inaccuracy \n",
    "\n",
    "  - *as is seen in this example taken from Keith Knight's STA410 [notes7.pdf](https://q.utoronto.ca/courses/296804/files?preview=24300633) document*\n",
    "  \n",
    "   $$A = \\left[\\begin{array}{cc}1&1-\\epsilon\\\\1+\\epsilon&1\\end{array}\\right] \\quad \\text{with analytical inverse} \\quad \n",
    "A^{-1} = \\left[\\begin{array}{cc}\\epsilon^{-2}&\\epsilon^{-1}-\\epsilon^{-2}\\\\-\\epsilon^{-1}-\\epsilon^{-2}&\\epsilon^{-2}\\end{array}\\right]$$\n",
    "  - and $\\det(A) = |A| = A_{11}A_{22} - A_{12}A_{21} = \\epsilon^{2} \\not = 0 $ so $1/\\det(A) = \\det(A^{-1}) \\not = 0$ so $A$ is mathematically ***invertible***\n",
    "   \n",
    "\n",
    "  - but if the magnitude of $\\epsilon^{-2}$ outranges that of $\\epsilon^{-1}$, the $\\epsilon^{-1}$ terms are lost due to ***roundoff error*** and so $A^{-1}$ can never be accurately represented since in that case \n",
    "\n",
    "  $$A^{-1} \\approx [A^{-1}]_c = \\left[ \\left[\\begin{array}{cc}\\epsilon^{-2}&\\epsilon^{-1}-\\epsilon^{-2}\\\\-\\epsilon^{-1}-\\epsilon^{-2}&\\epsilon^{-2}\\end{array}\\right] \\right]_c = \\left[\\begin{array}{cc}\\epsilon^{-2}&-\\epsilon^{-2}\\\\-\\epsilon^{-2}&\\epsilon^{-2}\\end{array}\\right]$$\n",
    "\n",
    "  so $\\det([A]_c)=0$ so $[A]_c$ is no longer ***invertible***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bwrYHLmvIKmm"
   },
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/2891790/how-to-pretty-print-a-numpy-array-without-scientific-notation-and-with-given-pre\n",
    "np.set_printoptions(precision=16)\n",
    "\n",
    "# For the matrix\n",
    "epsilon =  .5/100 #  2**-30 # which is not that extreme, e.g., 2**1023 # \n",
    "A = np.array([[1, 1-epsilon], \n",
    "              [1+epsilon, 1]])\n",
    "# (some other potentially helpful matrix functionality:\n",
    "#  e.g, np.ones, np.diag_indices, np.fill_diagonal, etc.)\n",
    "print(\"A\")\n",
    "print(A)\n",
    "\n",
    "print(\"Condition(A)\")\n",
    "print(np.linalg.cond(A))\n",
    "\n",
    "# The analytical inverse is\n",
    "A_inv = np.array([[epsilon**-2, 1/epsilon-epsilon**-2],\n",
    "                  [-1/epsilon-epsilon**-2, epsilon**-2]])\n",
    "print(\"\\n\\nA**-1\")\n",
    "print(A_inv)\n",
    "\n",
    "# Which can be confirmed\n",
    "print(\"\\n\\nA @ A_inv\")\n",
    "print(A @ A_inv) # matrix multiplication\n",
    "print(\"\\n\\nI\")\n",
    "print(np.eye(2)) # identity matrix\n",
    "print(\"\\n\\n(A @ A_inv) == I\")\n",
    "print(A @ A_inv == np.eye(2)) # Confirmation\n",
    "\n",
    "# However, this breaks because \n",
    "# (0) general roundoff error; but, even for numbers that exactly representable  \n",
    "# (1) the magnitude of epsilon**-2 will outrange epsilon**-1 if epsilon is small..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jANGECysLiLA"
   },
   "source": [
    "<a name=\"cell-sovling-SMWoodbury\"></a>\n",
    "\n",
    "### [POSTPONED?] Sherman-Morrison-Woodbury Formula ([Return to TOC](#cell-solving))\n",
    "\n",
    "---\n",
    "\n",
    "- *The presentation in this section is taken from Keith Knight's STA410 [notes7.pdf](https://q.utoronto.ca/courses/296804/files?preview=24300633) document*. \n",
    "\n",
    "Also known as the ***Woodbury Matrix Identity***,\n",
    "\n",
    "$$(A + UCV)^{−1} = A^{−1} − A^{−1}U (C^{−1} + VA^{−1}U)^{−1}VA^{−1}$$\n",
    "\n",
    "makes inversion simple if $A$ and $C$ are diagonal.\n",
    "\n",
    "Thus, ***low rank*** $m<n$ matrix approximations, with $A=I$ and $C=1$ \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\Sigma_{n \\times n}^{-1} \\approx {} & (I_{n \\times n} + \\mathbf{u}_{n\\times m}(\\mathbf{v}^T)_{n\\times m})^{-1}\\\\\n",
    "= {} & I - \\mathbf{u}(1+\\mathbf{v}^T\\mathbf{u})^{-1}\\mathbf{v}^T\\\\\n",
    "= {} & I - \\frac{\\mathbf{u}\\mathbf{v}^T}{1+\\mathbf{v}^T\\mathbf{u}} \\quad \\text{ if } m=1\n",
    "\\end{align*}$$\n",
    "\n",
    "can be used to trivialize matrix inversion approximation calculations.\n",
    "\n",
    "In fact, performing computations on the basis of this identity can even avoid numeric problems.  Returning to the example of the previous section \n",
    "\n",
    "$$A = \\left[\\begin{array}{cc}1 & 1 - \\epsilon\\\\ 1\n",
    "+ \\epsilon & 1 \\end{array}\\right] = \\left[\\begin{array}{cc}0 & - \\epsilon\\\\  \\epsilon & 0 \\end{array}\\right] + \\left[\\begin{array}{c}1 \\\\1 \\end{array}\\right]\n",
    "\\left[\\begin{array}{c}1 \\\\1 \\end{array}\\right]^T$$\n",
    "\n",
    "and $x=A^{-1}b$ is not a computation that will work to solve $Ax = b$ if $A^{-1}$ cannot be accurately computed; however, by instead computing\n",
    "\n",
    "$$\n",
    "x = A^{-1}b = \\left(\\left[\\begin{array}{cc}0 & - \\frac{1}{\\epsilon}\\\\  \\frac{1}{\\epsilon} & 0 \\end{array}\\right] - \n",
    "\\frac{\n",
    "\\left[\\begin{array}{cc}0 & - \\frac{1}{\\epsilon}\\\\  \\frac{1}{\\epsilon} & 0 \\end{array}\\right] \\left[\\begin{array}{c}1 \\\\1 \\end{array}\\right] \\left[\\begin{array}{c}1 \\\\1 \\end{array}\\right]^T \\left[\\begin{array}{cc}0 & - \\frac{1}{\\epsilon}\\\\  \\frac{1}{\\epsilon} & 0 \\end{array}\\right]  \n",
    "}{1 +  \\left[\\begin{array}{c}1 \\\\1 \\end{array}\\right]^T   \n",
    "\\left[\\begin{array}{cc}0 & - \\frac{1}{\\epsilon}\\\\  \\frac{1}{\\epsilon} & 0 \\end{array}\\right]\n",
    "\\left[\\begin{array}{c}1 \\\\1 \\end{array}\\right]}\\right) b$$\n",
    "\n",
    "$Ax = b$ can be accurately solved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "sET2_UvQVa9l"
   },
   "outputs": [],
   "source": [
    "ep = 1e-2 #5, 7, 8,9,12\n",
    "A = np.array([[1,1-ep],[1+ep,1]])\n",
    "A_inv = np.array([[ep**-2, 1/ep - ep**-2],[-1/ep + -ep**-2,  ep**-2]])\n",
    "b = np.array([[1],[1]])\n",
    "print(\"epsilon\", ep)\n",
    "print(\"Condition number\", np.linalg.cond(A))\n",
    "print(\"\\nA^-1 @ b = ?\")\n",
    "print(\"@ means matrix multiply\")\n",
    "\n",
    "print(\"\\nTrue Answer\")\n",
    "print(np.array([[1/ep],[-1/ep]])) \n",
    "print(\"\\nAnalytical Inverse\")\n",
    "print(A_inv@b)\n",
    "B = np.array([[0,-ep],[ep,0]])\n",
    "u = np.ones((2,1))\n",
    "v = u.T\n",
    "B_inv = np.linalg.inv(B)\n",
    "print(\"\\nWoodbury's Identity\")\n",
    "print((B_inv - (B_inv @ u @ v @ B_inv) / (1 + v @ B_inv @ u) ) @ b)\n",
    "print(\"\\nCalcluated Inverse\")\n",
    "print(np.linalg.inv(A) @ b)\n",
    "print(\"\\nLinear Equation Solver\")\n",
    "print(np.linalg.solve(A, b))\n",
    "print(\"\\nCalcluated Genearlized Inverse\")\n",
    "print(np.linalg.pinv(A) @ b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R-CFkzTkqWRs"
   },
   "source": [
    "<a name=\"cell-sovling-inverses\"></a>\n",
    "\n",
    "### [OMITTED] Generalized Inverses ([Return to TOC](#cell-solving))\n",
    "\n",
    "---\n",
    "\n",
    "Returning to our system of linear equations $Ax = b$, it is very straightforward to analyticall calculate $x = A^{-1} b$ for ***full rank square*** matrices $A$ if we have the ***eigendecomposition*** or ***SVD*** of $A$ since \n",
    "\n",
    "- if $A$ is a ***symmetric*** and ***full rank*** then $A = V \\Lambda V^T$ and $A^{-1} = V \\Lambda^{-1} V^T$ with $\\Lambda^{-1}_{ii} = \\frac{1}{\\Lambda_{ii}}$ since\n",
    "\n",
    "\\begin{align*}\n",
    "A^{-1}A = {} & V \\Lambda^{-1} V^T V \\Lambda V^T\\\\\n",
    "= {} & V \\Lambda^{-1} \\;\\;\\, I \\;\\;\\, \\Lambda V^T\\\\\n",
    "= {} & V \\quad \\;\\;\\, I \\;\\;\\, \\quad V^T = I\\\\\n",
    "\\end{align*}\n",
    "\n",
    "- if $A$ is ***square*** and ***full rank*** $A = U D V^T$ and $A^{-1} = V D^{-1} U^T$ with $D^{-1}_{ii} = \\frac{1}{D_{ii}}$ since\n",
    "\n",
    "\\begin{align*}\n",
    "A^{-1}A = {} & V^T D^{-1} U U^T D V\\\\\n",
    "= {} & V^T D^{-1} \\;\\;\\, I \\;\\;\\, D V\\\\\n",
    "= {} & V^T \\quad \\;\\;\\; I \\;\\;\\;\\quad V = I\\\\\n",
    "\\end{align*}\n",
    "\n",
    "Further, $Ax = b$ can be ***consistent*** (i.e., have at least one solution) even if $A$ is not squre or full rank.\n",
    "\n",
    "- $Ax = b$ is ***consistent*** if \n",
    "\n",
    "  $$\\text{rank}(A|b) = \\text{rank}(A)$$\n",
    "\n",
    "  where $A|b$ is the matrix made by appending the column $b$ as the rightmost column of $A$.\n",
    "\n",
    "If $Ax = b$ is ***consistent***, then a solution $x = A^{-}b$ based on $A^{-}$ only requires that \n",
    "\n",
    "$$A = A A^{-}A $$\n",
    "\n",
    "since\n",
    "\n",
    "\\begin{align*}\n",
    " Ax = {} & A A^{-}Ax\\\\\n",
    "  = {} & A A^{-}b\\\\\n",
    " \\Longrightarrow x = {} & A^- b \\quad \\text{ is a possible solution}\\\\\n",
    "\\end{align*}\n",
    "\n",
    "Matrices which can play the role of $A^{-}$ above, from strongest to weakest, are\n",
    "\n",
    "0. $A^{-1}$:  ***inverses*** which satisfy $A^{-1}A = AA^{-1} = I$\n",
    "1. $A^{+}$: (unique) ***Moore-Penrose inverses*** for which $A^{+}A$ and $AA^{+}$ are ***symmetric*** and which are also ***g1*** and ***g2 inverses***\n",
    "  > also called ***p-inverses***, ***normalized generalized inverses***, or ***pseudoinverses***\n",
    "2. $A^{*}$: ***g2 inverses*** for which $A^{*}AA^{*} = A^{*}$ and which are also ***g1 inverses***\n",
    "  > also called ***reflexive generalized inverses*** and ***outer pseudoinverses***\n",
    "3. $A^{-}$: ***g1 inverses*** for which $A A^{-}A = A$\n",
    "  > also called ***conditional inverses*** or ***inner pseudoinverses***\n",
    "\n",
    "Now, if $Ax = b$ is ***consistent***, then \n",
    "\n",
    "$$x = A^{+}b \\quad \\text{is a solution to} \\quad Ax = b$$\n",
    "\n",
    "where \n",
    "- $A^{+} = V D^{+} U^T$ and $D^{+}_{ii}=\\frac{1}{D_{ii}}$ is taken from the SVD $A = U D V^T$ \n",
    "and can be seen to satisfy $AA^{+}A = A$ and be the (unique) ***Moore-Penrose inverse***."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XifhxW0a0bgL"
   },
   "source": [
    "<a name=\"cell-sovling-notAxbwAinv\"></a>\n",
    "\n",
    "## 3.3.1 Not $A^{-1}$ ([Return to TOC](#cell-solving))\n",
    "\n",
    "---\n",
    "\n",
    "The `np.linalg.solve(A,b)` approach is more computationally efficient than `np.linalg.inv(A)@b`. So how does `np.linalg.solve(A,b)` work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VxTp-5dhHr2o"
   },
   "source": [
    "<a name=\"cell-sovling-backsub\"></a>\n",
    "\n",
    "### [PREREQUISITE] Backward Substitution ([Return to TOC](#cell-solving))\n",
    "\n",
    "---\n",
    "\n",
    "Consider the easier problem of solving for $x$ in $A_{n \\times n}x = b$ when $A_{n \\times n}$ is given in ***upper triangular form***, where everything below the diagonal is zero and everything on the diagonal is non-zero.\n",
    "\n",
    "$$\\left[\\begin{array}{cccccc} \n",
    "a_{11}&a_{12}&a_{13}& \\cdots & a_{1(n-1)} & a_{1n}\\\\\n",
    " &a_{22} &a_{23} & \\cdots &a_{2(n-1)} & a_{2n} \\\\ \n",
    " &&a_{33} & \\cdots &a_{3(n-1)} & a_{3n} \\\\ \n",
    " &&& \\ddots & \\vdots & \\vdots \\\\\n",
    "& &&& a_{(n-1)(n-1)}& a_{(n-1)n}\\\\\n",
    "0 & &&& & a_{nn}\\\\\n",
    "\\end{array}\\right] \n",
    "\\left[\\begin{array}{c} \n",
    "x_1\\\\x_2\\\\x_3\\\\\\vdots\\\\x_{n-1}\\\\x_{n}\\\\\n",
    "\\end{array}\\right] = \n",
    "\\left[\\begin{array}{c} \n",
    "b_1\\\\b_2\\\\b_3\\\\\\vdots\\\\b_{n-1}\\\\b_{n}\\\\\n",
    "\\end{array}\\right]$$\n",
    "\n",
    "In this form $x$ can be solved for using ***backward substitution*** as\n",
    "\n",
    "$$x_n = \\frac{b_n}{a_{nn}} \\quad x_{n-1} = \\frac{b_{n-1} - a_{(n-1)n}x_n}{a_{(n-1)(n-1)}} \\quad \\cdots \\quad x_{n-j} = \\frac{b_{n-j} - \\sum_{i=n}^{n-j+1}a_{(n-j)i}x_i}{a_{(n-j)(n-j)}}$$\n",
    "\n",
    "so long as (the so-called ***pivot points***) $a_{jj} \\neq 0$ so there is no division by zero. \n",
    "\n",
    "For $x_j$, the final formula shows that there is $1$ division and $n-j$ multiplications and $n-j$ subtractions, so the total number of arithmetic computations to solve for all $x_j$ is \n",
    "\n",
    "$$\\sum_{j=n}^1 1 + 2(n-j) = \\sum_{j=0}^{n-1} (1 + 2j) = n + 2 \\sum_{j=0}^{n-1} j = n + 2\\frac{n(n-1)}{2} = n^2$$\n",
    "\n",
    "> The presentation above is given for ***square invertible*** A; but, the ***backward substitution*** algorithm can also find solutions to ***non-square*** systems of equations based on $A_{n\\times m}$ were the ***upper triangular form*** is exchanged with [row echelon form](https://en.wikipedia.org/wiki/Row_echelon_form) (where every row must have more leading zeros than the row above it). A system $A_{n\\times m}x = b$ itself may be \n",
    "> \n",
    "> - ***overdetermined*** ($n>m$) with more equations (rows) than the unknown variables (and the \"triangle\" completes before the final row of $A$)\n",
    "> - ***underdetermined*** $(n<m)$ so there are more free unknown variables than the number of equations (and the triangle doesn't complete before before the final row of $A$)\n",
    "> \n",
    "> and the system $A_{n\\times m}x = b$ may be \n",
    "> \n",
    "> - ***consistent*** *with a single solution*, e.g.,    \n",
    ">\n",
    ">   $$\\left[\\begin{array}{cc}\n",
    "1 & 1\\\\\n",
    "0 & 1\n",
    "\\end{array}\\right]\n",
    "\\left[\\begin{array}{c}\n",
    "x_1\\\\\n",
    "x_2\n",
    "\\end{array}\\right] = \n",
    "\\left[\\begin{array}{c}\n",
    "1\\\\\n",
    "1\n",
    "\\end{array}\\right] \n",
    "$$\n",
    "> \n",
    ">   in which case $\\text{rank}(A) = \\text{rank}(A|b)$, and the columns (and rows) of $A$ are ***linearly independent*** so no columns (and rows) will be linear combinations of each other\n",
    "> \n",
    "> - ***consistent*** *with infinitely many solutions*, e.g., \n",
    ">\n",
    ">   $$\\left[\\begin{array}{cc}\n",
    "1 & 1\\\\\n",
    "0 & 0\n",
    "\\end{array}\\right]\n",
    "\\left[\\begin{array}{c}\n",
    "x_1\\\\\n",
    "x_2\n",
    "\\end{array}\\right] = \n",
    "\\left[\\begin{array}{c}\n",
    "1\\\\\n",
    "0\n",
    "\\end{array}\\right] \n",
    "$$\n",
    "> \n",
    ">   in which case $\\text{rank}(A) = \\text{rank}(A|b)$, but some columns (and rows) of $A$ are ***linearly dependent*** so some columns (and rows) will be linear cominations of each other\n",
    ">   \n",
    ">   > i.e., for some sets of indices $\\mathcal{J} = \\{j_k: k=1,...,K\\}$ and $\\mathcal{I} = \\{i_k: k=1,...,K\\}$\n",
    ">   >\n",
    ">   > $$\\sum_{j \\in \\mathcal{J}} c_j A_{*j} = 0  \\;\\; \\not \\! \\Longrightarrow  \\;\\; c_j = 0 \\quad \\text{ and } \\quad \\sum_{i \\in \\mathcal{I}} c_i A_{i*} = 0  \\;\\; \\not \\! \\Longrightarrow  \\;\\; c_i = 0$$\n",
    "> \n",
    "> - ***inconsistent*** *with no solutions at all*, e.g., \n",
    "> \n",
    ">   $$\\left[\\begin{array}{cc}\n",
    "1 & 1\\\\\n",
    "0 & 0\n",
    "\\end{array}\\right]\n",
    "\\left[\\begin{array}{c}\n",
    "x_1\\\\\n",
    "x_2\n",
    "\\end{array}\\right] = \n",
    "\\left[\\begin{array}{c}\n",
    "0\\\\\n",
    "1\n",
    "\\end{array}\\right] \n",
    "$$\n",
    "> \n",
    ">   in which case $\\text{rank}(A) < \\text{rank}(A|b)$, and the column $b$ cannot be constructed as a linear combination of the columns of $A$.\n",
    "> \n",
    "> where $A|b$ is the $n \\times (m+1)$ [*augmented matrix*](https://en.wikipedia.org/wiki/Augmented_matrix)\n",
    "> \n",
    "> $$\\left[\\begin{array}{ccc:c} \n",
    "a_{11} & \\cdots & a_{1m} & b_1 \\\\\n",
    "\\vdots & \\ddots & \\vdots  & \\vdots \\\\\n",
    "a_{n1} & \\cdots & a_{nm} & b_m \\end{array}\\right]$$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M4DzQtrJ3jc8"
   },
   "source": [
    "<a name=\"cell-sovling-elimination\"></a>\n",
    "\n",
    "## [PREREQUISITE] Gaussian Elimination ([Return to TOC](#cell-solving))\n",
    "\n",
    "---\n",
    "\n",
    "Converting a system of linear equations into ***upper triangular form*** (or the more general ***row echelon form***) is itself a quite simple process known as ***Gaussian elimination***.\n",
    "\n",
    "- Multiplying a row of the augmented matrix $A|b$ by a constant and adding to another row of the augmented matrix produces an equivalent system of linear equations to the one originally defined by the augmented matrix, i.e.,\n",
    "\n",
    "  $$x \\quad \\text{ solving } \\quad E^{ci+j}Ax = E^{ci+j}b \\quad \\text{ also solves } \\quad Ax = b$$\n",
    "\n",
    "- Multiplying and adding rows in this manner can produce leading zeros, e.g.,\n",
    "\n",
    "  $$\\left[\\begin{array}{ccc:c} \n",
    "a_{11} & \\cdots & a_{1m} & b_1 \\\\\n",
    "\\vdots & \\ddots & \\vdots  & \\vdots \\\\\n",
    "a_{n1} & \\cdots & a_{nm} & b_n \\end{array}\\right]\n",
    "\\quad \\overset{A|b \\; \\rightarrow \\; E^{c1+m}[A|b]}{\\longrightarrow} \\quad\n",
    "\\left[\\begin{array}{ccc:c} \n",
    "a_{11} & \\cdots & a_{1m} & b_1 \\\\\n",
    "\\vdots & \\ddots & \\vdots  & \\vdots \\\\\n",
    "a_{n1} + ca_{11} & \\cdots & a_{nm} +c a_{1m} & b_n + cb_1\\end{array}\\right]$$\n",
    "\n",
    "  and if $c = -\\frac{a_{n1}}{a_{11}}$ then $a_{n1} + ca_{11} = 0$ and the bottom left element of the resulting matrix vanishes (i.e., becomes $0$).\n",
    "\n",
    "When the column elements below a ***pivot point*** have been turned into zeros, the column and row of the ***pivot point*** are completed, the the ***Gaussian elimination*** process recurrsively restarts on the next ***pivot points*** in the top left corner of the submatrix without the completed row and column. \n",
    "\n",
    "$$\\left[\\begin{array}{c|ccc:c} \n",
    "a_{11} & a_{12} &  \\cdots & a_{1m} & b_1 \\\\\\hline\n",
    "0 & a_{22} + c_2 a_{2m} & \\cdots & a_{2m} + c_2 a_{1m} & b_2 + c_n b_1 \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots  & \\vdots \\\\\n",
    "0 & a_{n2} + c_n a_{n1} & \\cdots & a_{n2} + c_n a_{nm} & b_n + c_n b_1\\end{array}\\right]$$\n",
    "\n",
    "For a ***square*** matrix $A$ where $m=n$, the above formulation [shows](http://www.it.uom.gr/teaching/linearalgebra/chapt6.pdf) that the number of divisions and multiplication-additions that are required to create an ***upper triangular form*** matrix (augmented with the transformed $b$ column) are\n",
    "\n",
    "$$\\sum_{j=1}^n (j+1)(j-1) + \\underset{\\text{due to } b}{(j-1)} = \\sum_{j=1}^n j^2 - 1 + (j-1) = \\frac{n(n+1)(2n+1)}{6} - n + \\frac{n(n+1)}{2} - n $$\n",
    "\n",
    "> #### Pivoting\n",
    "> An important computational caveat is that when the scalar multiplier $c$ is large, the numerical precision of the floating point-operation will be insufficient if \n",
    "> $$[b_{i'} + cb_i]_c = [cb_i]_c$$ \n",
    "> e.g., for three digits of precision, one step of ***Gaussian elimination*** on \n",
    ">\n",
    "> \\begin{align*}\n",
    "0.0001 x_1 + x_2 & {} = 1\\\\\n",
    "x_1 + x_2 & {} =  2\\\\\n",
    "\\quad \\quad \\quad \\quad \\quad \\; \\text{produces } \\quad \\quad \\quad & {}    \\\\\n",
    " \\quad 0.0001 x_1 + x_2 & {} = 1\\\\\n",
    " -10000x_2 & {} = -10000 \\quad \\text{ (the \"$+x_2$\" and the 2 are lost due to precision!)}\\\\\n",
    "\\end{align*}\n",
    ">\n",
    "> To fix this issue the rows may be reordered with a ***partial pivot*** so $c$ will be as small as possible, and ***Gaussian elimination*** step will then be \n",
    ">\n",
    "> \\begin{align*}\n",
    "x_1 + x_2 & {} =  2\\\\\n",
    "0.0001 x_1 + x_2 & {} = 1\\\\\n",
    "\\text{instead produces } \\quad \\quad \\quad & {}    \\\\\n",
    "x_1 + x_2 & {} =  2\\\\\n",
    "x_2 & {} = 1 \\quad \\text{ (has roundoff error but solution's more accurate)}\\\\\n",
    "\\end{align*}\n",
    ">\n",
    "> which gives $x_1=x_2=1$ which is a more accurate solution than $x_1=0$ $x_2=1$.\n",
    "> \n",
    "> If this was not sufficient, a ***full pivot*** which reorders both the rows and the columns as well could be used to produce an even smaller $c$.\n",
    ">\n",
    "> *This example is inspired by the **Pivoting** subsection of the \n",
    "Section 5.2 **Gaussian Elimination and Elementary Operator Matrices** in Chapter 5 **Numerical Linear Algebra** on page 212 of James E. Gentle's **Computational Statistics** textbook.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nzJn5KGJ4OQw"
   },
   "source": [
    "<a name=\"cell-sovling-elementary\"></a>\n",
    "\n",
    "## [PREREQUISITE] Elementary Operations ([Return to TOC](#cell-solving))\n",
    "\n",
    "---\n",
    "\n",
    "Mathematically, multiplying row $i$ by scalar $c$ and adding it to row $j$, and ***partial pivoting*** two rows $i$ and $j$ are so-called ***elementary operations*** and are represented by simple matrix multiplications $E^{ci+j}[A|b]$ and $E^{i\\leftrightarrow j}[A|b]$, respectively, where\n",
    "\n",
    "- $E^{ci+j} = I + c e_je_i^T$, so $E^{ci+j}_{kk}=1$ and $E^{ci+j}_{ji}=c$ and all other entries of $E^{ci+j}$ are $0$.\n",
    "\n",
    "  - $\\left(E^{ci+j}\\right)^{-1} = E^{-ci+j}$ since $E^{ci+j} E^{(-c)i+j} =  E^{(-c)i+j}E^{ci+j} = I$.\n",
    "\n",
    "- $E^{i\\leftrightarrow j} = I^{i\\leftrightarrow j}$ where row $i$ and $j$ have in the identity matrix $I$ have been switched, so all $E^{i\\leftrightarrow j}_{kk}=1$ except $E^{i\\leftrightarrow j}_{ii} = E^{i\\leftrightarrow j}_{jj} = 0$ and all other elements are $0$ except $E^{i\\leftrightarrow j}_{ij}=E^{i\\leftrightarrow j}_{ji}=1$.\n",
    "\n",
    "  - $(E^{i\\leftrightarrow j})^{-1} = E^{i\\leftrightarrow j}$ since $E^{i\\leftrightarrow j}E^{i\\leftrightarrow j}=I$ so it's (of course) easy to \"undo\" row interchanges.\n",
    "\n",
    "\n",
    "$$\n",
    "E^{ci+j} = \\left[\\begin{array}{ccccccc}\n",
    "1 & &&&&&0\\\\\n",
    "&1&&&&&\\\\\n",
    " && \\ddots&&\\\\\n",
    "& && 1 &&\\\\\n",
    " &&c&& \\ddots\\\\\n",
    "&&\\uparrow&&&1&\\\\\n",
    "0&&E^{ci+j}_{ji}&&&&1\\\\\n",
    "\\end{array}\\right] \n",
    "\\quad\\quad\n",
    "E^{i\\leftrightarrow j} = \\left[\\begin{array}{ccccccc}\n",
    "1 &0&&&&0&0\\\\\n",
    "0& \\ddots &&&&&0\\\\\n",
    " &\\cdots & 0 &\\cdots& 1&\\cdots \\\\\n",
    " &&& \\ddots \\\\\n",
    " &\\cdots & 1 &\\cdots& 0 &\\cdots \\\\\n",
    " 0 &&&&& \\ddots &0\\\\    \n",
    "0  &0&&&&0& 1 \\\\  \n",
    "\\end{array}\\right]\n",
    "\\begin{array}{c}\\leftarrow \\text{ row }i\\\\\\\\\\leftarrow \\text{ row }j\\\\\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "igiOaBvJ3zZt"
   },
   "source": [
    "\n",
    "<a name=\"cell-sovling-lu\"></a>\n",
    "\n",
    "### 3.3.1.0 The LU Decomposition ([Return to TOC](#cell-solving))\n",
    "\n",
    "---\n",
    "\n",
    "Ignoring row interchanges $E^{i\\leftrightarrow j}$ which are easily applied and undone, ***Gaussian elimination*** transformation sequence \n",
    "\n",
    "$$\\prod E^{ci+j} \\quad \\text{and} \\quad \\left(\\prod E^{ci+j}\\right)^{-1} = \\prod E^{-ci+j} = L$$ \n",
    "\n",
    "will be ***lower triangular matrices*** and \n",
    "\n",
    "  $$U = \\left(\\prod E^{ci+j}\\right) A$$ \n",
    "\n",
    "[may](https://math.stackexchange.com/questions/218770/when-does-a-square-matrix-have-an-lu-decomposition/2274657) (if ***Gaussian elimination*** is working) be an ***upper triangular***, and when so\n",
    "\\begin{align*}\n",
    "Ax = {} & b\\\\\n",
    "\\left(\\prod E^{ic+j}\\right) Ax = {} & \\left(\\prod E^{ic+j}\\right) b\\\\\n",
    "Ux = {} & L^{-1}b\n",
    "\\end{align*}\n",
    "\n",
    "and $x$ may be solved for by simple ***backward substitution***.\n",
    "\n",
    "***LU decomposition*** is thus seen to be a byproduct of solving for $x$ using ***Gausian elimination*** where operation is done using the extended augmated matrix [$A|I|b$](https://en.wikipedia.org/wiki/Gaussian_elimination#Finding_the_inverse_of_a_matrix) instead of only $A|b$  \n",
    "\n",
    "$$\\left[\\begin{array}{ccc:ccc:c} \n",
    "a_{11} & \\cdots & a_{1m} & 1 & \\cdots & 0 & b_1 \\\\\n",
    "\\vdots & \\ddots & \\vdots  & \\vdots & \\ddots & \\vdots & \\vdots  \\\\\n",
    "a_{n1} & \\cdots & a_{nm} & 0 & \\cdots & 1 & b_n \\end{array}\\right]\n",
    "\\quad \\overset{[A|I|b] \\;\\rightarrow \\;L^{-1}[A|I|b]}{\\longrightarrow} \\quad \n",
    "\\left[ \\!\\begin{array}{c:c:c}  U & L^{-1} & b' \\!\\end{array}  \\right]$$\n",
    "\n",
    "Computationally all that needs to be kept track of is the sequence of the ***elementary operations*** $\\left(\\prod E^{ic+j}\\right)$ actualized during the ***Gausian elimination*** process, since \n",
    "\n",
    "$$U = \\left(\\prod E^{ic+j}\\right) A \\quad \\text{and} \\quad\n",
    "\\left(\\prod E^{ic+j}\\right)I = L^{-1} \\quad \\text{and} \\quad\n",
    "\\left(\\prod E^{ic+j}\\right)b = b'$$\n",
    "\n",
    "> The ***LU decomposition*** will be [unique](https://math.stackexchange.com/questions/1799854/is-the-l-in-lu-factorization-unique) if\n",
    ">\n",
    "> $$x^TAx \\geq 0 \\quad\\quad \\text{ subject to the constraint } \\quad \\quad L_{kk} = 1 \\; \\text{ or } \\; U_{kk} = 1 \\; \\text{ for all $k$}$$\n",
    ">\n",
    "> i.e., if $A$ is a ***square nonnegative definite matrix***, and the diagonals of either $L$ or $U$ all one."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
