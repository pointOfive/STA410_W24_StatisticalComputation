{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x0jz-iiBbGxA"
   },
   "source": [
    "<a name=\"cell-TOC-opt\"></a>\n",
    "\n",
    "### 8. [Optimizers: Newton's Method from various pragmatic perspectives](#cell-opt-fx-)\n",
    "\n",
    "1. [Altering Newton's Methods](#cell-opt-fx-3)\n",
    "2. [Newton-Like Methods](#cell-opt-fx-newtonlike)\n",
    "3. [Ensuring Monotonic Iteration](#cell-opt-fx-newton-like-ascent)\n",
    "4. [Quasi-Newton Methods](#cell-opt-fx-quasi-newton)\n",
    "5. [Gradient Methods](#cell-opt-fx-common-optimization-algorithms)\n",
    "6. [Hessian Diagonal Alternatives](#cell-opt-fx-common-optimization-algorithms2)\n",
    "7. [Nelder-Mead](#cell-opt-fx-nelder-mead)\n",
    "\n",
    "\n",
    "<!--\n",
    "1. ~Combinatorial (Discrete) Optimzation~\n",
    "    - ~Simulated Annealing~\n",
    "2. ~Constrained optimization~\n",
    "    - ~Expectation-Maximization~\n",
    "    - ~Interior and Exterior Point Algorithms~\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QZTVcIVNncC3"
   },
   "source": [
    "\n",
    "<a name=\"cell-opt-fx-3\"></a>\n",
    "\n",
    "## 8.1 Altering Newton's Methods ([Return to TOC](#cell-TOC-opt)) \n",
    "\n",
    "---\n",
    "\n",
    "***Iterative methods*** which optimize $g(x^{(t)})$ are ***fixed point iteration methods*** and take the form $x^{(t+1)} = x^{(t)} + \\alpha h(x^{(t)})$ for $\\alpha \\neq 0$. E.g., for ***positive definite Hessian*** $H_{g(x)}$ examples include\n",
    "\n",
    "- ***Newton's method*** $x^{(t+1)} = x^{(t)} - \\alpha H_{g(x)}^{-1}(x^{(t)})\\nabla_x g(x^{(t)})$ with ***learning rate*** $\\alpha$\n",
    "  > which converges if $||\\alpha H_{g(x)}^{-1}(x^{(t)})H_{g(x)}(x^*)||=\\lambda_\\max < 1$ which is true for constant $H_{g(x)}$ and $\\alpha<1$ since then $\\left|\\left| \\alpha H_{g(x)}^{-1}(x^{(t)}) H_{g(x)}(x^*) \\right|\\right| = ||\\alpha I||<1$\n",
    "\n",
    "- ***Gradient Descent*** $x^{(t+1)} = x^{(t)} - \\alpha I \\nabla_x g(x^{(t)})$\n",
    "  > which (following from the above) converges if $||\\alpha H_{g(x)}(x^*)||=\\lambda_\\max < 1$; and, the smaller $\\left|\\left| I - \\alpha H_{g(x)}(x^*) \\right|\\right| < 1$ the faster the convergence will be\n",
    "\n",
    "- or most generally $x^{(t+1)} = x^{(t)} - \\alpha [M^{(t)}]^{-1}\\nabla_x g(x^{(t)})$ \n",
    "  > which converges if $||\\alpha [M^{(t)}]^{-1}H_{g(x)}(x^*)||=\\lambda_\\max < 1$; and, the smaller $\\left|\\left| I - \\alpha [M^{(t)}]^{-1}H_{g(x)}(x^*) \\right|\\right| < 1$ the faster the convergence will be\n",
    "\n",
    "The convergence statements made here were shown and considered more closely in `STA410_W24_Week7_Extra_NewtonVariantsConvergence.ipynb`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x0ZE7wArcFui"
   },
   "source": [
    "<a name=\"cell-opt-fx-newton-like-ascent\"></a>\n",
    "\n",
    "## 8.3 Ensuring Monotonic Iteration ([Return to TOC](#cell-TOC-opt)) \n",
    "\n",
    "---\n",
    "\n",
    "***Newton-like updates*** $x^{(t+1)} = x^{(t)} - \\alpha M^{-1}\\nabla_x g(x^{(t)})$ move towards $x^*$ such that $\\nabla_x g(x^*) = 0$, but only in a  monotonically decreasing (increasing) manner if $M$ is ***positive*** (***negative***) ***definite*** and some sufficiently small ***learning rate*** $\\alpha \\neq 0$.\n",
    "\n",
    "Since \n",
    "- $M+cI$ will (have all positive ***eigenvalues*** and) be positive definite if $c>0$ is large enough   \n",
    "- $M+cI$ will (have all negative ***eigenvalues*** and) be negative definite if $c<0$ is small enough \n",
    "\n",
    "  > and these forms of definiteness could also be achieved with diagonal $D_c$ with $[D_c]_{ii} = c_i$ which minimizes alteration of the original $M$ through the so-called [***modified Cholesky decomposition***](https://nhigham.com/2020/12/22/what-is-a-modified-cholesky-factorization/)\n",
    "\n",
    "***modified Newton methods*** guarantee monotonic convergence by instead updating\n",
    "\n",
    "$$x^{(t+1)} = x^{(t)} - \\alpha \\underset{\\text{or } M+D_c}{(M+cI)}^{-1}\\nabla_x g(x^{(t)})$$\n",
    "\n",
    "This guarantees monotonic $g(x^{(t+1)})<g(x^{(t)})$ since $(M+cI)^{-1}\\nabla_x g(x^{(t)})$ will have the same signs as $\\nabla_x g(x^{(t)})$ and so \n",
    "$$g\\left(x^{(t)} - \\alpha {(M+cI)}^{-1}\\nabla_x g(x^{(t)})\\right)$$\n",
    "\n",
    "must be less than $g(x^{(t)})$ for some small $\\alpha>0$.\n",
    "\n",
    "> Since ***gradient descent*** follows the direction of steepest descent of $g$ at $x^{(t)}$ (by moving in the negative direction of the gradient) for some small step size factor $\\alpha^{(t)} > 0$, since $I$ is positive definite, we will have that $g(x^{(t+1)}) < g(x^{(t)})$.\n",
    "\n",
    "After ensuring the necessary ***positive (negative)definiteness***, the ***step size factor*** $\\alpha$ can be found by the ***line search method*** of ***backtracking***. If monotonic convergence is violated for a specific $\\alpha^{(t)}$ then is can be made smaller."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N9dBEPmioiGQ"
   },
   "source": [
    "<a name=\"cell-opt-fx-quasi-newton\"></a>\n",
    "\n",
    "## 8.4 Quasi-Newton Methods ([Return to TOC](#cell-TOC-opt)) \n",
    "\n",
    "---\n",
    "\n",
    "As noted previously, the naive generalization of the ***secant method*** to scalar valued multivariate functions as a discrete approximation to ***Newton's method*** does not offer any computational incentives over simply computing the Hessian itself.  However, for \n",
    "\n",
    "- sequential iterations $\\quad x^{(t)}$ and $x^{(t-1)}$\n",
    "- sequential gradients  $\\quad \\nabla_{x}g(x^{(t)})$ and $\\nabla_{x}g(x^{(t-1)})$\n",
    "\n",
    "an $M^{(t)}$ satisfying the so-called ***secant condition***\n",
    "\n",
    "$$ \\underbrace{\\nabla_{x}g(x^{(t)}) - \\nabla_{x}g(x^{(t-1)})}_{\\Delta^{(t)}_{\\nabla_{x}g}} = M^{(t)}\\underbrace{(x^{(t)} - x^{(t-1)})}_{\\Delta_x^{(t)}}$$\n",
    "\n",
    "provides the discrete ***secant*** approximation of the Hessian where\n",
    "\n",
    "$$x^{(t+1)} = x^{(t)} - \\left[M^{(t)}\\right]^{-1}\\nabla_x g(x^{(t)}) \\quad \\text{ replaces } \\quad x^{(t+1)} = x^{(t)} - \\left[H_{g(x)}(x^{(t)})\\right]^{-1}\\nabla_x g(x^{(t)})$$\n",
    "\n",
    "and where $M^{(t)}$ which satisfies the ***secant condition*** can be derived iteratively on the basis of $M^{(t-1)}$ which itself already satisfies the ***secant condition***. This is shown in the following material, but understanding that there are available calculations here is more important than understanding the specific computations themsevles.\n",
    "\n",
    "> This ***secant condition*** alone does not provide an efficient calculation of $M^{(t)}$, but the (computationally inexpensive) ***rank-one update*** [derived here](https://personal.math.ubc.ca/~loew/m604/web-ho/sr1.pdf)\n",
    ">\n",
    "> $$M^{(t)} = M^{(t-1)} + \\underbrace{\\frac{v^{(t)}[v^{(t)}]^T}{[v^{(t)}]^T\\Delta_x^{(t)}c}}_{\\text{rank-one update}} \\quad \\text{ where } \\quad v^{(t)} = \\left(\\Delta^{(t)}_{\\nabla_{x}g} - M^{(t-1)}\\Delta_x^{(t)}\\right)$$\n",
    ">\n",
    "> results in $M^{(t)}$ which satisfies the ***secant condition***, subject to the following caveats.\n",
    "> - If the denomenator $[v^{(t)}]^T\\Delta_x^{(t)} \\approx 0$, the update might need to be skipped by setting $M^{(t+1)} = M^{(t)}$.\n",
    "> - If the denomenator $[v^{(t)}]^T\\Delta_x^{(t)}<0$ $(>0)$ and $M^{(t-1)}$ is ***negative*** $($***positive***$)$ ***definite***, then $M^{(t)}$ will be as well.\n",
    ">   - Thus, this update only guarantees ***hereditary positive $($negative$)$ definiteness*** under the above conditions; however,\n",
    ">   - scaling $[v^{(t)}]^T\\Delta_x^{(t)}$ in the denominator by some large factor $c$ so the update contributes less to $M^{(t)}$ can maintain the definiteness state.\n",
    "\n",
    "> A ***rank-two version*** of the above which both satisfies the ***secant condition*** and confers ***hereditary definiteness*** is the so-called ***BFGS*** update (named after its authors). The (*rank-two*) ***BFGS*** update is just the (*rank-two*) ***Broyden class update*** \n",
    "> \n",
    "> $$\\begin{align*} M^{(t)} = {} & M^{(t-1)} - \\frac{M^{(t-1)}\\Delta_x^{(t)} [M^{(t-1)}\\Delta_x^{(t)}]^T}{[\\Delta_x^{(t)}]^TM^{(t-1)}\\Delta_x^{(t)}} + \\frac{\\Delta^{(t)}_{\\nabla_{x}g}[\\Delta^{(t)}_{\\nabla_{x}g}]^T}{[\\Delta_x^{(t)}]^T\\Delta^{(t)}_{\\nabla_{x}g}} + \\delta^{(t)}\\left([\\Delta_x^{(t)}]^TM^{(t-1)}\\Delta_x^{(t)} \\right)[d^{(t)}]^Td^{(t)}\\\\\n",
    "{} & \\text{where } d^{(t)} = \\frac{\\Delta^{(t)}_{\\nabla_{x}g}}{[\\Delta_x^{(t)}]^T\\Delta^{(t)}_{\\nabla_{x}g}} - \\frac{M^{(t-1)}\\Delta_x^{(t)}}{[\\Delta_x^{(t)}]^TM^{(t-1)}\\Delta_x^{(t)}}\n",
    "\\end{align*}$$\n",
    ">\n",
    "> with $\\delta^{(t)}=0$. \n",
    "\n",
    "> A few points to note regarding ***quasi-Newton methods*** are:\n",
    "> - many authors find the ***rank-one update*** to have superior performance to ***Broyden class updates***, including ***BFGS***\n",
    "- the above ***BFGS*** update is numerically unstable, and is better approached through a ***Cholesky decomposition*** \n",
    ">   - ***quasi-Newton methods*** are very sensitive to the scale of the $x_i$ comprising $x$, with performance tending to be better for similarly scaled $x_i$\n",
    ">   - ***quasi-Newton methods*** are very sensitive to the initial choice $M^{(0)}$ though for similarly scaled $x_i$ starting with $I$ (for minimization) or $-I$ (for maximization) is usually sufficient; however, in maximum likelihood estimation contexts starting with $-I(\\theta^{(0)})$ is usually a better choice\n",
    "- the ***observed information*** (i.e., the negative Hessian) provides a point estimate of the ***precision*** (i.e., inverse covariance) structure of $p(\\hat \\theta) \\approx N(\\theta, \\Sigma^{-1} = -H_{l(\\theta)}(\\hat \\theta))$, but quasi-Newton methods  (intentionally) do not provide close estimates of the Hessian; so, for statistical purposes, re-estimating the ***observed information*** upon convergence is an obligatory final step, e.g., with the ***central difference approximation***\n",
    ">\n",
    ">   $$ \\widehat{[H_{l(\\theta)}(\\theta^{(t)})]}_{ij} = \\frac{[\\nabla_\\theta l(\\theta^{(t)} + h_{ij}e_j)]_i - [\\nabla_\\theta l(\\theta^{(t)} - h_{ij}e_j)]_i}{2h_{ij}} $$\n",
    ">   perhaps with $h_{ij} = h = \\epsilon^{\\frac{1}{3}}$ where $\\epsilon$ is the available computer precision. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "akApiXS1TNyf"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import fmin_l_bfgs_b\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers.legacy import SGD, Adagrad, RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jmz4h2aUUq1x",
    "outputId": "2b19db2f-ddc9-4673-b0c5-8cf27a8e0c2b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.96574895,  0.52026429,  0.89394714, -0.96574895,  0.52026429,\n",
       "         0.89394714, -0.09902554, -0.09902554, -0.2844477 , -0.2844477 ,\n",
       "         0.11431975,  0.11431975,  0.0948675 ,  0.0948675 , -1.86349271,\n",
       "        -0.2773882 , -0.35475898]),\n",
       " 4.94918505807411e-09,\n",
       " {'grad': array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.04897387,\n",
       "         0.7243671 , 0.68328701]),\n",
       "  'task': 'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH',\n",
       "  'funcalls': 972,\n",
       "  'nit': 24,\n",
       "  'warnflag': 0})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(3)\n",
    "alpha,K = 0.01,10\n",
    "d,q1,q2 = 3,2,3\n",
    "# these are the data\n",
    "x = np.random.normal(size=(d,1))\n",
    "y = np.random.normal(size=(d,1))\n",
    "# these are all the parameters\n",
    "A1 = np.random.normal(size=(q1,d))\n",
    "b1 = np.random.normal(size=(q1,1))\n",
    "A2 = np.random.normal(size=(q2,q1))\n",
    "b2 = np.random.normal(size=(q2,1))\n",
    "\n",
    "# the parameters are passed into the function as a vector\n",
    "# https://stackoverflow.com/questions/8672005/correct-usage-of-fmin-l-bfgs-b-for-fitting-model-parameters\n",
    "def objective(parameters):\n",
    "    # parameters get unpacked into their model form\n",
    "    A1 = parameters[0:(q1*d)].reshape(q1,d)\n",
    "    b1 = parameters[(q1*d):(q1*d+q1)].reshape(q1,1)\n",
    "    A2 = parameters[(q1*d+q1):(q1*d+q1+q2*q1)].reshape(q2,q1)\n",
    "    b2 = parameters[(q1*d+q1+q2*q1):].reshape(q2,1)\n",
    "    # this is the model form\n",
    "    x1 = A1@x+b1\n",
    "    x1 = x1*(x1>0)\n",
    "    x2 = A2@x1+b2\n",
    "    # here's the residual from the prediction of this model\n",
    "    epsilon = y-x2\n",
    "    # and here's the loss function\n",
    "    return epsilon.T.dot(epsilon)[0,0]**0.5\n",
    "\n",
    "# https://stackoverflow.com/questions/8672005/correct-usage-of-fmin-l-bfgs-b-for-fitting-model-parameters\n",
    "fmin_l_bfgs_b(func=objective, x0=np.ones(q1*d+q1+q2*q1+q2), approx_grad=True, m=4)\n",
    "# showing this for a latent dimension of 4 (1 or 2 does not work)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p0UR0mJ4aWGP",
    "outputId": "b22a5b47-e367-47c2-c48f-7152ef5b26eb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.8634927 , -1.86349271],\n",
       "       [-0.2773882 , -0.2773882 ],\n",
       "       [-0.35475898, -0.35475898]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Indeed this converges for BFGS with rank 4\n",
    "fit_parameters = fmin_l_bfgs_b(func=objective, x0=np.ones(q1*d+q1+q2*q1+q2), approx_grad=True, m=4)[0]\n",
    "A1 = fit_parameters[0:(q1*d)].reshape(q1,d)\n",
    "b1 = fit_parameters[(q1*d):(q1*d+q1)].reshape(q1,1)\n",
    "A2 = fit_parameters[(q1*d+q1):(q1*d+q1+q2*q1)].reshape(q2,q1)\n",
    "b2 = fit_parameters[(q1*d+q1+q2*q1):].reshape(q2,1)\n",
    "x1 = A1@x+b1\n",
    "x1 = x1*(x1>0)\n",
    "x2 = A2@x1+b2\n",
    "np.c_[y,x2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y_1S_UjKhpGK",
    "outputId": "08fe7de9-a549-4efc-e874-d742bd714ebd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[1.18176167]], shape=(1, 1), dtype=float64)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(3, 1), dtype=float64, numpy=\n",
       " array([[-1.52092405],\n",
       "        [ 0.37256329],\n",
       "        [ 0.57085996]])>,\n",
       " <tf.Tensor: shape=(3, 1), dtype=float64, numpy=\n",
       " array([[-1.8634927 ],\n",
       "        [-0.2773882 ],\n",
       "        [-0.35475898]])>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# These popular iterative optimization techniques are readily\n",
    "# available in modern computational frameworks like tensorflow\n",
    "\n",
    "np.random.seed(3)\n",
    "alpha,K = 0.01,10\n",
    "d,q1,q2 = 3,2,3\n",
    "x = tf.constant(np.random.normal(size=(d,1)))\n",
    "y = tf.constant(np.random.normal(size=(d,1)))\n",
    "A1 = tf.Variable(np.random.normal(size=(q1,d)))\n",
    "b1 = tf.Variable(np.random.normal(size=(q1,1)))\n",
    "A2 = tf.Variable(np.random.normal(size=(q2,q1)))\n",
    "b2 = tf.Variable(np.random.normal(size=(q2,1)))\n",
    "\n",
    "@tf.function()\n",
    "def objective():\n",
    "    x1 = A1@x+b1\n",
    "    x1 = x1*tf.cast(x1>0, tf.float64)\n",
    "    x2 = A2@x1+b2\n",
    "    epsilon = y-x2   \n",
    "    return tf.tensordot(tf.transpose(epsilon), epsilon, axes=1)**(0.5)\n",
    "\n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/schedules\n",
    "alpha_t, epsilon_t = 0.1,0.5\n",
    "sgd = SGD(learning_rate=alpha_t, momentum=epsilon_t, nesterov=True)\n",
    "adagrad = Adagrad(learning_rate=alpha_t)\n",
    "rho_t = 0.5\n",
    "rmsprop = RMSprop(learning_rate=alpha_t, rho=rho_t, momentum=epsilon_t)\n",
    "rho_v_t, rho_s_t = epsilon_t, rho_t\n",
    "adam = Adam(learning_rate=alpha_t, beta_1=rho_v_t, beta_2=rho_s_t)\n",
    "\n",
    "steps = 10\n",
    "for t in range(steps):\n",
    "  sgd.minimize(objective, var_list=[A1, b1, A2, b2])\n",
    "  #adagrad.minimize(objective, var_list=[A1, b1, A2, b2])\n",
    "  #rmsprop.minimize(objective, var_list=[A1, b1, A2, b2])\n",
    "  #adam.minimize(objective, var_list=[A1, b1, A2, b2])\n",
    "print(objective())\n",
    "\n",
    "x1 = A1@x+b1\n",
    "x1 = x1*tf.cast(x1>0, tf.float64)\n",
    "x2 = A2@x1+b2\n",
    "x2,y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RmR6E2Nlmzfs"
   },
   "source": [
    "<a name=\"cell-opt-fx-common-optimization-algorithms\"></a>\n",
    "\n",
    "## 8.5 Gradient Methods ([Return to TOC](#cell-TOC-opt)) \n",
    "\n",
    "---\n",
    "\n",
    "***Gradient methods*** dispense with focussing on the second order derivatives of the ***Hessian*** and focus on improving computational performance using only the first order derivatives of the ***gradient*** $\\nabla_\\theta g(\\theta)$. This makes a lot of sense in the context of ***non-convex*** functions where the ***Hessian*** $H_{g(\\theta)}$ cannot reliably support convergence towards a global optimum. And, since many modern optimization contexts involve ***non-convex*** functions, ***gradient methods*** have become ubiquitous features of the modern optimization toolkit. \n",
    "\n",
    "> Even for ***convex*** $g(\\theta)$ the compelling argument for ***gradient methods*** over ***Newton's method*** is that they may be more computationally efficient overall.  If the computation of the ***Hessian*** is expensive relative to the computations of the ***gradient*** (as is generally the case), then skipping the ***Hessian*** computation means more update steps based on cheaper ***gradient*** computations can be made, and this may end up being more computationally efficient overall. \n",
    "\n",
    "The following ***gradient methods*** are frequently encountered in modern optimization contexts.\n",
    "\n",
    "- ***Stochastic Gradient Descent*** $x^{(t+1)} = x^{(t)} - \\alpha I \\widehat{\\nabla_x g(x^{(t)})}$\n",
    "  - replaces the Hessian $H_{g(\\theta)}$ with the identity matrix $I$\n",
    "  - uses a step size factor $\\alpha_t$ which may evolve according to a prescribed schedule\n",
    "  - and makes steps using gradients estimated from small ***batches*** (e.g., $m=32$ observations) rather than all of the available data\n",
    "\n",
    "    $$\\frac{1}{m} \\sum_{i=1}^m \\nabla_\\theta g_{x_i}(\\theta_{t-1}) = \\nabla_\\theta \\frac{1}{m} \\sum_{i=1}^m g_{x_i}(\\theta_{t-1}) \\quad \\text{ estimates } \\quad E_x\\left[ \\nabla_\\theta g_x(\\theta_{t-1}) \\right]$$\n",
    "<!-- = \\nabla_\\theta g(\\theta_{t-1}) -->\n",
    "\n",
    "  Sequences of ***batches*** constructed from the full data comprise one ***epoch***, and a sequence of ***stochastic gradient descent*** steps are often constructed from multiple ***epochs***, i.e., many passes through many batches of data.\n",
    "\n",
    "  > ***Stochastic gradient descent*** drastically reduces computation because roughly accurate estimates of the gradients can be easily computed without having to use all the data.\n",
    "  >\n",
    "  > More than that, however, estimating gradients from batches is empirically observed to outperform calculating gradients based on the full data set. \n",
    "  > - ***Stochastic gradient descent*** introduces noise into the iterative trajectory which increases the exploration potential of the $\\theta_t$ sequence, making it less likely to coverge (***overfit***) on a suboptimal local minima; and, even within an attractive region of a local minima the $\\theta_t$ sequence is ***regularized*** in the sense that it will never actually achieve the local minima value since the estimated gradient will be different for each ***batch*** of data. \n",
    "\n",
    "  \n",
    "\n",
    "- ***Momentum*** replaces the gradient with a running average using prescribed ***learning rate*** $\\alpha_t$ and ***historic decay*** weighting schedules $\\epsilon_t$ as\n",
    "\n",
    "  $$\\begin{align*}\n",
    "  v_t = {} & \\epsilon_t v_{t-1} + \\alpha_t \\nabla_\\theta \\frac{1}{m} \\sum_{i=1}^m g_{x_i}(\\theta_{t-1})\\\\\n",
    "  \\theta_t = {} & \\theta_{t-1} - v_t\n",
    "  \\end{align*}$$\n",
    "\n",
    "  - ***Neterov Momentum*** is a slight variant based on the \"look ahead\" update $$\\begin{align*}\n",
    "  v_t = {} & \\epsilon_t v_{t-1} + \\alpha_t \\nabla_\\theta \\frac{1}{m} \\sum_{i=1}^m g_{x_i}(\\theta_{t-1} + \\epsilon_{t} v_{t-1})\n",
    "  \\end{align*}$$\n",
    "\n",
    "    though this typically has equivalent performance to the original ***momentum*** specification.\n",
    "\n",
    "  The idea of ***momentum*** is to use the ***gradient*** history information instead of the ***Hessian*** at each update step to predict the shape of the function being optimized.  The assumption underlying ***momentum*** is that previous ***gradients*** are good estimates of future ***gradients***. \n",
    "\n",
    "> ***Momentum*** takes the form\n",
    ">\n",
    "> $$v_t = \\sum_{j=1}^{t} \\underbrace{\\left[\\prod_{k=j}^{t-1} \\epsilon_{k+1} \\right]}_{1 \\text{ if } k=t\\;>\\;t-1} \\alpha_j \\underbrace{\\nabla_\\theta \\frac{1}{m} \\sum_{i=1}^m g_{x_i}(\\theta_{j-1})}_{\\widehat{\\nabla_\\theta g(\\theta_{j-1})}}$$\n",
    ">\n",
    "> for which there exists some vector $d_t$ that accounts for the difference between $v_t$ and $\\epsilon_t v_{t-1}$ such that\n",
    ">\n",
    "> $$ v_t = \\alpha_t \\underbrace{d_t \\odot \\overbrace{\\nabla_\\theta \\frac{1}{m} \\sum_{i=1}^m g_{x_i}(\\theta_{t-1})}^{\\widehat{\\nabla_\\theta g(\\theta_{t-1})}}}_{\\text{element-wise multiplication}}$$\n",
    ">\n",
    "> The ***momentum*** update \n",
    "> \n",
    "> $$\\theta_t = \\theta_{t-1} - \\alpha_t d_t \\odot \\widehat{\\nabla_\\theta g(\\theta_{t-1})} = \\underbrace{\\theta_{t-1} - \\alpha_t D_t^{-1} \\widehat{\\nabla_\\theta g(\\theta_{t-1})}}_{\\text{expressed as diagonal matrix } D_t^{-1}}$$ \n",
    ">\n",
    "> can thus be viewed as a diagonal alternative to ***Newton's method*** where the $D_t$ replaces the diagonal elements of the ***Hessian*** $H_{g(\\theta)}$ with values based on the decay weighted history of ***gradients*** which force a trajectory that is a decay weighted average of the ***gradient*** history."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i8uxNG3nfDJ2"
   },
   "source": [
    "<a name=\"cell-opt-fx-common-optimization-algorithms2\"></a>\n",
    "\n",
    "## 8.6 Hessian Diagonal Alternatives ([Return to TOC](#cell-TOC-opt)) \n",
    "\n",
    "---\n",
    "\n",
    "***Diagonal approximations*** $H_{g(\\theta)} \\approx I \\circ H_{g(\\theta)} = D_\\gamma$ which ignore the off-diagonal elements of the Hessian attempt to approximate the second order partial derivatives as \n",
    "\n",
    "$$[D_{\\gamma}]_{ii} = \\frac{\\partial^2 g (\\theta)}{\\partial\\theta_i\\partial\\theta_i} \\approx \\gamma_i \\quad \\text{ and } \\quad [D_{\\gamma}]_{ij} = \\frac{\\partial^2 g( \\theta)}{\\partial\\theta_i\\partial\\theta_j} \\approx 0$$ \n",
    "\n",
    "and approximate ***Newton's method*** as \n",
    "\n",
    "$$ \\theta^{(t+1)} = \\theta^{(t)} - D_\\gamma^{-1} \\nabla_\\theta g(\\theta^{(t)}) $$\n",
    "\n",
    "As the example of ***momentum*** above shows, other alternatives for the diagonal of the ***Hessian*** are possible.\n",
    "\n",
    "- ***Adagrad*** enables coordinate-specific learning rates by weighting the elements of a gradient step inversely to the magnitude of the step sizes accumulated along the axis so far as\n",
    "\n",
    "  $$\\begin{align*}\n",
    "  g_t = {} & \\nabla_\\theta \\frac{1}{m} \\sum_{i=1}^m g_{x_i}(\\theta_{t-1})\\\\\n",
    "  r_t = {} & r_{t-1} + g_t \\odot g_t \\quad \\text{  element-wise product}\\\\\n",
    "  \\theta_t = {} & \\theta_{t-1} - \\frac{\\alpha_t}{\\delta_t+\\sqrt{r_t}} \\odot g_t \\quad \\text{  element-wise square root, division, and product}\n",
    "  \\end{align*}$$\n",
    "\n",
    "  where the square root transforms the sum back to original units, and $\\delta_t>0$ ensures division by $0$ is avoided. \n",
    "  \n",
    "  > The effect of the method is to make step sizes smaller along fast-moving axes and larger along slow-moving axes.\n",
    "\n",
    "\n",
    "- **RMSprop** replaces the accumulation in ***Adagrad*** with a decaying running average\n",
    "\n",
    "  $$\\begin{align*}\n",
    "  r_t = {} & \\rho r_{t-1} + (1-\\rho) g_t \\odot g_t\n",
    "  \\end{align*}$$\n",
    "\n",
    "  which allows for the application of locally variying coordinate-specific learning rates.\n",
    "\n",
    "  ***RMSprop*** also admits the subsequent incorporation of ***Momentum*** as $$\\begin{align*}\n",
    "  v_t = {} & \\epsilon_t v_{t-1} + \\frac{\\alpha_t}{\\delta_t+\\sqrt{r_t}} \\odot g_t\\\\\n",
    "  \\theta_t = {} & \\theta_{t-1} - v_t\n",
    "  \\end{align*}$$\n",
    "\n",
    "  > ***RMSprop*** provides both the coordinate-specific learning rates of  ***Adagrad*** as well as incorporating a ***momentum*** effect.\n",
    "\n",
    "- **Adam** is a slight variant of ***RMSprop*** which directly incorporates ***momentum*** as another decaying running average\n",
    "\n",
    "  $$\\begin{align*}\n",
    "  g_t = {} & \\nabla_\\theta \\frac{1}{m} \\sum_{i=1}^m g_{x_i}(\\theta_{t-1})\\\\\n",
    "  v_t = {} & \\rho_v v_{t-1} + (1-\\rho_v) g_t \\quad\\quad \\times \\frac{1}{1-\\rho_v^t} \\text{ to correct if } v_0 = 0  \\\\\n",
    "  r_t = {} & \\rho_s r_{t-1} + (1-\\rho_s) g_t \\odot g_t \\quad \\quad \\times  \\frac{1}{1-\\rho_s^t} \\text{ to correct if } r_0=0 \\\\\n",
    "  \\theta_t = {} & \\theta_{t-1} - \\frac{\\alpha_t}{\\delta_t+\\sqrt{r_t}} \\odot v_t\n",
    "  \\end{align*}$$\n",
    "\n",
    "  > The performance of ***Adam*** is often equivalent to that of ***RMSprop*** with ***momentum***. \n",
    "\n",
    "> Emtiyaz Khan characterizes these common algorithms as varying degrees of approximation to \"Bayesian Learning Rules\" in this [presentation](https://slideslive.com/38923183/deep-learning-with-bayesian-principles), starting around slide 60."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xvuRb9s8VSqJ"
   },
   "source": [
    "<a name=\"cell-opt-fx-nelder-mead\"></a>\n",
    "\n",
    "## 8.7 Nelder-Mead ([Return to TOC](#cell-TOC-opt)) \n",
    "\n",
    "---\n",
    "\n",
    "***Newton's method*** is not the only tool in the optimization toolbox.  \n",
    "\n",
    "For example ***Gauss-Seidel*** was previously encountered and seen to be equivalent to ***coordinate descent***; and, ***non-linear Gauss-Seidel*** was seen to naturally generalize ***Gauss-Seidel*** to nonlinear functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xqJHwdBQgqMp"
   },
   "source": [
    "\n",
    "So far most of the focus has been on understanding slopes and curvature of functions in order to iteratively \"hill climb\" towards optima. This has been predominantly done through the use of gradiants and hessians; however, the previously introduced ***bisection*** and ***ternary search*** methods were able to find the roots and optima of nonlinear functions in a gradient-free manner. Another such method is the ***Nelder-Mead algorithm***, which prescribes a gradient-free optimization approach by substituing a (slow yet) robust heuristic algorithm in place of potentially expensive and unstable derivative computations.  \n",
    "\n",
    "- Gradient-based methods are great when they work since they specifically leverage information about the function in question that is helpful for solving the problem at hand; however, when they fail, the more robust gradient-free methods present a very attractive, albeit more computationally demanding alternative.\n",
    "- Gradient-free methods are generally more robust than their gradient-based counterparts; thus, when the computation required for gradient-free methods is tractable, they present a more general solution that gradient-based methods\n",
    "\n",
    "Here are some fun visualations illustrating the ***Nelder-Mead method***. The ***Nelder-Mead algorithm*** is relatively simple (and widely available, e.g., fully detailed in both James E. Gentle's as well as the Givens and Hoeting *Computational Statistics* textbooks); though, certainly it requires a careful and attentive implementation.   \n",
    "\n",
    "| | |\n",
    "|-|-|\n",
    "|![](http://takashiida.floppy.jp/wp-content/uploads/2020/11/05NelderMead-1.gif)| ![](https://userpages.umbc.edu/~rostamia/2020-09-math625/images/nelder-mead.gif) |\n",
    "| [Takashi Ida's Personal Website](http://takashiida.floppy.jp/en/education-2/gif-nelder-mead/) |  [Rouben Rostamian's (UMBC) Comp Math + C](https://userpages.umbc.edu/~rostamia/2020-09-math625/) |\n",
    "| ![](https://upload.wikimedia.org/wikipedia/commons/thumb/d/de/Nelder-Mead_Himmelblau.gif/640px-Nelder-Mead_Himmelblau.gif)| ![](https://upload.wikimedia.org/wikipedia/commons/0/08/Nelder_Mead2vectorised.gif) |\n",
    "| [Wikepedia's Nelder-Mead page](https://en.wikipedia.org/wiki/Nelder%E2%80%93Mead_method) |\n",
    "| ![](http://fab.cba.mit.edu/classes/864.14/students/Calisch_Sam/10/img/nelder-mead.gif) | ![](https://upload.wikimedia.org/wikipedia/commons/1/14/Direct_search_BROYDEN.gif) | \n",
    "| [Sam Calisch's homework submission for MIT MAS 864](http://fab.cba.mit.edu/classes/864.14/students/Calisch_Sam/10/img/nelder-mead.gif) | [Wikipedia's Pattern Search page (a different but similar method)](https://en.wikipedia.org/wiki/Pattern_search_(optimization))|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kakkz8gjg9I_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gqjiu3vX_G2D"
   },
   "source": [
    "<!--\n",
    "\n",
    "Quantile Regression\n",
    "\n",
    "lasso\n",
    "\n",
    "SGD\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OeBIFfpHg5Kk"
   },
   "source": [
    "<!--\n",
    "\n",
    "Quantile Regression\n",
    "\n",
    "lasso\n",
    "\n",
    "SGD\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F4yjGu9Ng3Sy"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
